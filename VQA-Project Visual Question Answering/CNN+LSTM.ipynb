{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### THUC HIEN TREN GG COLAB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxHSGFMJgdR1"
      },
      "source": [
        "Cai dat thu vien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "benUm87ZXmaP",
        "outputId": "68e4d990-c623-4474-a6bf-2ce0a70f535d"
      },
      "outputs": [],
      "source": [
        "!pip uninstall torchtext torch -y\n",
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 -f https://download.pytorch.org/whl/cu118/torch_stable.html\n",
        "!pip install torchtext==0.15.2 --no-cache-dir\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Giai nen file zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "relwogInldQb",
        "outputId": "25bcb7d8-a456-4585-9c86-3c2345ed4bf8"
      },
      "outputs": [],
      "source": [
        "!unzip vqa_coco_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgpfjS7xarHN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMGXPhMFgus7"
      },
      "source": [
        "cai dat gia tri ngau nhien co dinh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jKGJrNyguVi"
      },
      "outputs": [],
      "source": [
        "def set_seed (seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "seed = 59\n",
        "set_seed(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxcQtrHthJ20"
      },
      "source": [
        "chia bo du lieu train val test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZf2K1gChGMz"
      },
      "outputs": [],
      "source": [
        "# Load train data\n",
        "train_data = []\n",
        "train_set_path = './vaq2.0.TrainImages.txt'\n",
        "\n",
        "with open (train_set_path, \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    temp = line.split('\\t')\n",
        "    qa = temp[1].split('?')\n",
        "    if len(qa) == 3:\n",
        "      answer = qa[2].strip()\n",
        "    else:\n",
        "      answer = qa[1].strip()\n",
        "\n",
        "    data_sample = {\n",
        "      'image_path': temp[0][: -2],\n",
        "      'question': qa[0] + '?',\n",
        "      'answer': answer\n",
        "    }\n",
        "    train_data.append(data_sample)\n",
        "\n",
        "# Load val data\n",
        "val_data = []\n",
        "val_set_path = './vaq2.0.DevImages.txt'\n",
        "\n",
        "with open (val_set_path, \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    temp = line.split('\\t')\n",
        "    qa = temp[1].split('?')\n",
        "    if len(qa) == 3:\n",
        "      answer = qa[2].strip()\n",
        "    else:\n",
        "      answer = qa[1].strip()\n",
        "\n",
        "    data_sample = {\n",
        "      'image_path': temp[0][: -2],\n",
        "      'question': qa[0] + '?',\n",
        "      'answer': answer\n",
        "    }\n",
        "    val_data.append(data_sample)\n",
        "\n",
        "# Load test data\n",
        "test_data = []\n",
        "test_set_path = './vaq2.0.TestImages.txt'\n",
        "\n",
        "with open (test_set_path, \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    temp = line.split('\\t')\n",
        "    qa = temp[1].split('?')\n",
        "    if len(qa) == 3:\n",
        "      answer = qa[2].strip()\n",
        "    else:\n",
        "      answer = qa[1].strip()\n",
        "\n",
        "    data_sample = {\n",
        "      'image_path': temp[0][: -2],\n",
        "      'question': qa[0] + '?',\n",
        "      'answer': answer\n",
        "    }\n",
        "    test_data.append(data_sample)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NzsDpAMn9qX"
      },
      "source": [
        "Xay dung bo tu vung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KINJct4Rn9Ew"
      },
      "outputs": [],
      "source": [
        "eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_tokens(data_iter):\n",
        "  for sample in data_iter:\n",
        "    question = sample['question']\n",
        "    yield[token.text for token in eng.tokenizer(question)]\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "  get_tokens(train_data),\n",
        "  min_freq=2,\n",
        "  specials = ['<pad>', '<sos>', '<eos >', '<unk>'],\n",
        "  special_first=True\n",
        ")\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWQ_E0d1o1_0"
      },
      "source": [
        "Xay dung dictionary mapping classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZWlMOiPo6qu"
      },
      "outputs": [],
      "source": [
        "classes = set([sample ['answer'] for sample in train_data])\n",
        "label2idx = {\n",
        "  cls_name: idx for idx, cls_name in enumerate(classes)\n",
        "}\n",
        "\n",
        "idx2label = {\n",
        "  idx: cls_name for idx, cls_name in enumerate(classes)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKPBM5hXpOY3"
      },
      "source": [
        "Xay dung ham tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfs7-alNpQxl"
      },
      "outputs": [],
      "source": [
        "def tokenize (question, max_seq_len):\n",
        "  tokens = [token.text for token in eng.tokenizer(question)]\n",
        "  sequence = [vocab[token] for token in tokens]\n",
        "  if len(sequence) < max_seq_len:\n",
        "    sequence += [vocab['<pad>']] * (max_seq_len - len(sequence))\n",
        "  else:\n",
        "    sequence =sequence[:max_seq_len]\n",
        "\n",
        "  return sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYuXKNASpth6"
      },
      "source": [
        "Xay dung class Pytorch dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bcvk6GgLpwd8"
      },
      "outputs": [],
      "source": [
        "class VQADataset (Dataset):\n",
        "  def __init__(\n",
        "    self,\n",
        "    data,\n",
        "    label2idx,\n",
        "    max_seq_len =20,\n",
        "    transform=None,\n",
        "    img_dir='val2014-resised/'\n",
        "  ):\n",
        "    self.transform = transform\n",
        "    self.data = data\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.img_dir = img_dir\n",
        "    self.label2idx = label2idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img_path = os.path.join(self.img_dir, self.data[index]['image_path'])\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    if self.transform:\n",
        "      img = self.transform (img)\n",
        "    question = self.data[index]['question']\n",
        "    question = tokenize(question, self.max_seq_len)\n",
        "    question = torch.tensor(question, dtype=torch.long)\n",
        "    label = self.data[index]['answer']\n",
        "    label = label2idx[label]\n",
        "    label = torch. tensor(label, dtype=torch. long)\n",
        "    return img, question, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkK_ZSJ7qq9r"
      },
      "source": [
        "Xay dung Pytorch transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvkiD9CuraHG"
      },
      "outputs": [],
      "source": [
        "data_transform = {\n",
        "  'train': transforms.Compose ([\n",
        "    transforms.Resize(size=(224, 224)),\n",
        "    transforms.CenterCrop(size=180),\n",
        "    transforms.ColorJitter(brightness =0.1, contrast =0.1, saturation=0.1),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.GaussianBlur(3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "  ]),\n",
        "  'val': transforms.Compose ([\n",
        "    transforms.Resize(size=(224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "  ])\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXVUXsAMsfaE"
      },
      "source": [
        "Khai bao dataset object cho ba bo train val test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aWu0J5Askcy"
      },
      "outputs": [],
      "source": [
        "train_dataset = VQADataset(\n",
        "  train_data,\n",
        "  label2idx=label2idx,\n",
        "  transform=data_transform['train']\n",
        ")\n",
        "val_dataset = VQADataset(\n",
        "  val_data,\n",
        "  label2idx=label2idx,\n",
        "  transform=data_transform['val']\n",
        ")\n",
        "test_dataset = VQADataset(\n",
        "  test_data,\n",
        "  label2idx=label2idx,\n",
        "  transform=data_transform['val']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHfq-HtFsyL3"
      },
      "source": [
        "Khai bao DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1STsZZ7Ns0fJ"
      },
      "outputs": [],
      "source": [
        "train_batch_size = 256\n",
        "test_batch_size = 32\n",
        "\n",
        "train_loader = DataLoader (\n",
        "  train_dataset,\n",
        "  batch_size=train_batch_size,\n",
        "  shuffle=True\n",
        ")\n",
        "val_loader = DataLoader (\n",
        "  val_dataset,\n",
        "  batch_size=test_batch_size,\n",
        "  shuffle=False\n",
        ")\n",
        "test_loader = DataLoader (\n",
        "  test_dataset,\n",
        "  batch_size=test_batch_size,\n",
        "  shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaW845NEtGPs"
      },
      "source": [
        "Xay dung model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HmuSokvtH_o",
        "outputId": "816b2c0c-0375-4aa6-9ad6-cc8b55cc4e97"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "class VQAModel(nn.Module):\n",
        "  def __init__ (\n",
        "    self,\n",
        "    n_classes,\n",
        "    img_model_name,\n",
        "    embeddding_dim,\n",
        "    n_layers=2,\n",
        "    hidden_size=256,\n",
        "    drop_p=0.2\n",
        "  ):\n",
        "    super(VQAModel, self).__init__()\n",
        "    self.image_encoder = timm.create_model(\n",
        "      img_model_name,\n",
        "      pretrained=True,\n",
        "      num_classes = hidden_size\n",
        "    )\n",
        "    for param in self.image_encoder.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "    self.embedding = nn.Embedding(len(vocab), embeddding_dim)\n",
        "    self.lstm1 = nn.LSTM(\n",
        "        input_size=embeddding_dim,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=n_layers,\n",
        "        batch_first=True,\n",
        "        bidirectional=True,\n",
        "        dropout=drop_p\n",
        "    )\n",
        "\n",
        "    self.fc1 = nn.Linear(hidden_size * 3, hidden_size)\n",
        "    self.dropout = nn.Dropout(drop_p)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.fc2 = nn.Linear(hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, img, text):\n",
        "    img_features = self.image_encoder(img)\n",
        "\n",
        "    text_emb = self.embedding(text)\n",
        "    lstm_out, _ = self.lstm1(text_emb)\n",
        "\n",
        "    lstm_out = lstm_out[:, -1, :]\n",
        "\n",
        "    combined = torch.cat((img_features, lstm_out), dim=1)\n",
        "    x = self.fc1(combined)\n",
        "    x = self.gelu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "n_classes = len(classes)\n",
        "img_model_name = 'resnet18'\n",
        "hidden_size = 256\n",
        "n_layers = 2\n",
        "embeddding_dim = 128\n",
        "drop_p = 0.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = VQAModel(\n",
        "  n_classes=n_classes,\n",
        "  img_model_name=img_model_name,\n",
        "  embeddding_dim=embeddding_dim,\n",
        "  n_layers=n_layers,\n",
        "  hidden_size=hidden_size,\n",
        "  drop_p=drop_p\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zru14xuxvVaO"
      },
      "source": [
        "Xay dung ham train va evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ILbxqTXxmXi"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  losses = []\n",
        "  with torch.no_grad():\n",
        "    for image, question, labels in dataloader:\n",
        "      image, question, labels =image.to(device), question.to(device), labels.to(device)\n",
        "      outputs = model(image, question)\n",
        "      loss = criterion(outputs, labels)\n",
        "      losses. append(loss.item())\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  loss = sum (losses) / len (losses)\n",
        "  acc = correct / total\n",
        "\n",
        "  return loss, acc\n",
        "def fit (\n",
        "  model,\n",
        "  train_loader,\n",
        "  val_loader,\n",
        "  criterion,\n",
        "  optimizer,\n",
        "  scheduler,\n",
        "  device,\n",
        "  epochs\n",
        "):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  for epoch in range (epochs):\n",
        "    batch_train_losses = []\n",
        "\n",
        "    model.train()\n",
        "    for idx, (images, questions, labels) in enumerate( train_loader):\n",
        "      images = images.to(device)\n",
        "      questions = questions.to(device)\n",
        "      labels = labels.to (device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images, questions)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      batch_train_losses.append(loss.item())\n",
        "\n",
        "    train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    val_loss, val_acc = evaluate(\n",
        "      model, val_loader,\n",
        "      criterion, device\n",
        "    )\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print (f'EPOCH {epoch + 1}: \\tTrain loss: {train_loss:.4f}\\tVal loss: {val_loss:.4f}\\tVal Acc: {val_acc}')\n",
        "    scheduler.step()\n",
        "\n",
        "  return train_losses, val_losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OafJLOyN0eEa"
      },
      "source": [
        "Khai bao ham loss optimizer scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LY-rZ_wr0irh"
      },
      "outputs": [],
      "source": [
        "lr = 1e-3\n",
        "epochs = 50\n",
        "scheduler_step_size = epochs * 0.8\n",
        "\n",
        "criterion = nn.CrossEntropyLoss ()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "  model.parameters(),\n",
        "  lr=lr\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR (\n",
        "  optimizer,\n",
        "  step_size=scheduler_step_size,\n",
        "  gamma=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYXap9lS1Gln"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED8RhlDb1IeE",
        "outputId": "2a977739-3ae1-4ee0-f028-0e30fd59f704"
      },
      "outputs": [],
      "source": [
        "train_losses, val_losses = fit(\n",
        "  model,\n",
        "  train_loader,\n",
        "  val_loader,\n",
        "  criterion,\n",
        "  optimizer,\n",
        "  scheduler,\n",
        "  device,\n",
        "  epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ExM1_T31Zad"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db-6HtMF1b6c"
      },
      "outputs": [],
      "source": [
        "val_loss, val_acc = evaluate(\n",
        "  model,\n",
        "  val_loader,\n",
        "  criterion,\n",
        "  device\n",
        ")\n",
        "test_loss, test_acc = evaluate(\n",
        "  model,\n",
        "  test_loader,\n",
        "  criterion,\n",
        "  device\n",
        ")\n",
        "\n",
        "print('Evaluation on val/test dataset')\n",
        "print('Val accuracy: ' val_acc)\n",
        "print('Test accuracy: ', test_acc)\n",
        "\n",
        "## Evaluation results from author\n",
        "# Val accuracy: 0.4989754098360656\n",
        "# Test accuracy: 0.5084075173095944\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
